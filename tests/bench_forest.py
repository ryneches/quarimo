"""
tests/bench_forest.py
=====================================
pytest-benchmark benchmarks for Forest.

Purpose
-------
Establish baseline timings for the three primary cost phases so that
subsequent optimisations (Cython _rmq_csr, CUDA _quartet_kernel, parallel
construction) can be compared against reproducible numbers.

Phases measured
---------------
1. Construction          — NEWICK parsing + Euler tour + sparse table build
                           parameterised by n_trees × n_leaves
2. quartet_topology      — counts-only kernel
                           parameterised by n_trees × n_leaves × n_quartets
3. quartet_topology      — with Steiner distances (steiner=True)
                           same axes as (2)
4. branch_distance       — single pair across all trees
                           parameterised by n_trees × n_leaves

Design notes
------------
* No hard-coded NEWICK strings.  All trees are generated by two pure-Python
  functions (balanced binary tree and caterpillar) that accept explicit
  parameters.  The same functions produce the CSV expectation data used by
  the correctness tests so there is one source of truth.

* No magic numbers.  Every tunable constant is a named module-level value
  with a comment explaining its role.

* Scale parameters are chosen to keep each benchmark under ~5 s on a
  single CPU core at baseline so that the suite finishes in reasonable time
  on a laptop.  Larger scales are gated behind a --large-scale CLI flag.

* All fixtures use scope='module' so expensive collections are built once
  per benchmark session, not once per repetition.  The benchmark callable
  itself does only the work being measured.

Running
-------
    # fast default set
    pytest tests/bench_forest.py --benchmark-only

    # include large-scale marks
    pytest tests/bench_forest.py --benchmark-only -m "not large_scale"
    pytest tests/bench_forest.py --benchmark-only  # all marks

    # compare against a saved baseline
    pytest tests/bench_forest.py --benchmark-compare

    # save a new baseline
    pytest tests/bench_forest.py --benchmark-save=baseline
"""

from __future__ import annotations

import itertools
import os
import random
import sys

import pytest

_HERE = os.path.dirname(__file__)
_ROOT = os.path.dirname(_HERE)
sys.path.insert(0, _ROOT)

from forest import Forest, _NUMBA_AVAILABLE


# ======================================================================== #
# Tunable constants                                                         #
# ======================================================================== #

# Branch length used for all generated edges.  A uniform value keeps Steiner
# distances simple to reason about; we don't need biological realism here.
BRANCH_LENGTH: float = 1.0

# Random seed for taxon-name shuffling so benchmarks are reproducible.
RANDOM_SEED: int = 42

# Scale tiers for n_trees — (label, count) pairs used as pytest parameters.
TREE_COUNTS_SMALL: list[tuple[str, int]] = [
    ("trees10", 10),
    ("trees100", 100),
    ("trees1000", 1000),
]
TREE_COUNTS_LARGE: list[tuple[str, int]] = [
    ("trees5000", 5000),
    ("trees10000", 10000),
]

# Scale tiers for n_leaves per tree.
LEAF_COUNTS: list[tuple[str, int]] = [
    ("leaves10", 10),
    ("leaves50", 50),
    ("leaves200", 200),
]

# Number of quartets queried in the quartet_topology benchmarks.
# "all" is replaced at fixture-build time by C(n_leaves, 4) capped at MAX_QUARTETS.
QUARTET_COUNTS: list[tuple[str, int | str]] = [
    ("q1", 1),
    ("q100", 100),
    ("q_all", "all"),  # C(n_leaves, 4), capped below
]

# Maximum number of quartets in the "all" tier so benchmarks stay tractable.
MAX_QUARTETS: int = 5_000

# When building "all-quartets" lists, draw this many leaves to form quartets
# rather than the full leaf set — avoids C(200,4) = 64M combinations.
MAX_LEAVES_FOR_ALL_QUARTETS: int = 20


# ======================================================================== #
# Tree generators                                                           #
# ======================================================================== #


def _leaf_name(i: int) -> str:
    """
    Map a zero-based leaf index to a short, unique taxon name.

    Leaves 0-25 map to single letters A-Z; beyond that the pattern extends
    as AA, AB, …, ZZ, AAA, … ensuring names are always valid identifiers.
    """
    alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    if i < len(alphabet):
        return alphabet[i]
    # Encode as base-26 with at least two characters
    name = []
    i += 1  # shift so i=0 maps to A rather than being a terminator
    while i > 0:
        i, remainder = divmod(i - 1, len(alphabet))
        name.append(alphabet[remainder])
    return "".join(reversed(name))


def balanced_newick(
    n_leaves: int,
    branch_length: float = BRANCH_LENGTH,
    leaf_names: list | None = None,
) -> str:
    """
    Generate a perfectly balanced binary tree NEWICK string for *n_leaves*
    leaves.  If *n_leaves* is not a power of two, the tree is filled level by
    level left-heavy with any remaining slots becoming leaves early.

    Parameters
    ----------
    n_leaves : int
        Number of leaf taxa.  Must be >= 2.
    branch_length : float
        Uniform branch length assigned to every edge.
    leaf_names : list[str] | None
        Explicit taxon names in left-to-right leaf order.  If None, names are
        generated as A, B, C, … via _leaf_name().

    Returns
    -------
    str
        A valid NEWICK string ending in ';'.
    """
    if n_leaves < 2:
        raise ValueError(f"n_leaves must be >= 2, got {n_leaves}")
    names = (
        leaf_names
        if leaf_names is not None
        else [_leaf_name(i) for i in range(n_leaves)]
    )
    if len(names) != n_leaves:
        raise ValueError(f"leaf_names has {len(names)} entries, expected {n_leaves}")

    def _build(ns):
        if len(ns) == 1:
            return f"{ns[0]}:{branch_length}"
        mid = len(ns) // 2
        return f"({_build(ns[:mid])},{_build(ns[mid:])}):{branch_length}"

    return _build(names) + ";"


def caterpillar_newick(
    n_leaves: int,
    branch_length: float = BRANCH_LENGTH,
    leaf_names: list | None = None,
) -> str:
    """
    Generate a caterpillar (maximally unbalanced) binary tree NEWICK string.

    The caterpillar attaches one leaf at each internal node along a spine,
    which gives the worst-case Euler-tour depth for LCA queries.

    Parameters
    ----------
    n_leaves : int
        Number of leaf taxa.  Must be >= 2.
    branch_length : float
        Uniform branch length assigned to every edge.
    leaf_names : list[str] | None
        Explicit taxon names.  If None, defaults to A, B, C, …

    Returns
    -------
    str
        A valid NEWICK string ending in ';'.
    """
    if n_leaves < 2:
        raise ValueError(f"n_leaves must be >= 2, got {n_leaves}")
    names = (
        leaf_names
        if leaf_names is not None
        else [_leaf_name(i) for i in range(n_leaves)]
    )
    if len(names) != n_leaves:
        raise ValueError(f"leaf_names has {len(names)} entries, expected {n_leaves}")

    # Build from the tip inward: start with the two deepest leaves,
    # then wrap each successive leaf around the outside.
    tree = f"({names[-2]}:{branch_length},{names[-1]}:{branch_length})"
    for name in reversed(names[:-2]):
        tree = f"({name}:{branch_length},{tree}:{branch_length})"
    return tree + ";"


def generate_collection_newicks(
    n_trees: int,
    n_leaves: int,
    *,
    tree_type: str = "balanced",
    branch_length: float = BRANCH_LENGTH,
    seed: int = RANDOM_SEED,
) -> list:
    """
    Return a list of *n_trees* NEWICK strings, all with *n_leaves* leaves.

    Each tree is built with a shuffled permutation of the global leaf-name
    set so that the collection exercises the global_to_local mapping, while
    every tree's leaf set is identical (all taxa present in all trees).

    Parameters
    ----------
    n_trees : int
    n_leaves : int
    tree_type : "balanced" | "caterpillar" | "mixed"
        "mixed" alternates balanced and caterpillar trees so the benchmark
        exercises both LCA depth profiles in one collection.
    branch_length : float
    seed : int
        Seed for the leaf-name shuffler — different trees in the same
        collection have varied but reproducible local leaf orderings.

    Returns
    -------
    list[str]
    """
    rng = random.Random(seed)
    canonical = [_leaf_name(i) for i in range(n_leaves)]

    newicks = []
    for i in range(n_trees):
        # Shuffle the name list so each tree has the same taxa but different
        # local ordering — exercises global_to_local without missing taxa.
        shuffled = canonical[:]
        rng.shuffle(shuffled)

        if tree_type == "mixed":
            gen = balanced_newick if i % 2 == 0 else caterpillar_newick
        elif tree_type == "balanced":
            gen = balanced_newick
        elif tree_type == "caterpillar":
            gen = caterpillar_newick
        else:
            raise ValueError(f"Unknown tree_type {tree_type!r}")

        newicks.append(gen(n_leaves, branch_length=branch_length, leaf_names=shuffled))

    return newicks


def generate_quartet_list(
    leaf_names: list[str],
    n_quartets: int | str,
    seed: int = RANDOM_SEED,
) -> list[tuple[str, str, str, str]]:
    """
    Return a list of *n_quartets* 4-tuples sampled from *leaf_names*.

    Parameters
    ----------
    leaf_names : list[str]
    n_quartets : int | "all"
        If "all", return all C(|leaf_names|, 4) combinations up to
        MAX_QUARTETS.  If int, sample that many randomly (with replacement
        if n_quartets exceeds the number of available combinations).
    seed : int

    Returns
    -------
    list[tuple[str, str, str, str]]
    """
    if len(leaf_names) < 4:
        raise ValueError(
            f"Need at least 4 leaves to form a quartet, got {len(leaf_names)}"
        )

    all_combos = list(itertools.combinations(leaf_names, 4))

    if n_quartets == "all":
        return all_combos[:MAX_QUARTETS]

    rng = random.Random(seed)
    if n_quartets <= len(all_combos):
        return rng.sample(all_combos, n_quartets)
    # More quartets requested than available combinations — sample with replacement.
    return [rng.choice(all_combos) for _ in range(n_quartets)]


# ======================================================================== #
# Shared collection fixtures (module-scoped — built once per session)      #
# ======================================================================== #


@pytest.fixture(
    scope="module", params=[v for _, v in LEAF_COUNTS], ids=[k for k, _ in LEAF_COUNTS]
)
def n_leaves(request):
    """Parametrised leaf count fixture."""
    return request.param


@pytest.fixture(
    scope="module",
    params=[v for _, v in TREE_COUNTS_SMALL],
    ids=[k for k, _ in TREE_COUNTS_SMALL],
)
def n_trees_small(request):
    """Small tree-count tiers for fast benchmarks."""
    return request.param


@pytest.fixture(
    scope="module",
    params=[v for _, v in TREE_COUNTS_SMALL + TREE_COUNTS_LARGE],
    ids=[k for k, _ in TREE_COUNTS_SMALL + TREE_COUNTS_LARGE],
)
def n_trees_all(request):
    """All tree-count tiers including large-scale."""
    return request.param


@pytest.fixture(scope="module")
def collection_balanced_small(n_trees_small, n_leaves):
    """
    Pre-built balanced-tree collection for query benchmarks.

    Scope=module means this is constructed once and reused across all
    benchmark repetitions that share the same (n_trees, n_leaves) parameters.
    The construction cost itself is measured separately.
    """
    newicks = generate_collection_newicks(n_trees_small, n_leaves, tree_type="balanced")
    return Forest(newicks)


@pytest.fixture(scope="module")
def collection_caterpillar_small(n_trees_small, n_leaves):
    """Pre-built caterpillar-tree collection for query benchmarks."""
    newicks = generate_collection_newicks(
        n_trees_small, n_leaves, tree_type="caterpillar"
    )
    return Forest(newicks)


@pytest.fixture(scope="module")
def collection_mixed_small(n_trees_small, n_leaves):
    """Pre-built mixed (balanced + caterpillar) collection for query benchmarks."""
    newicks = generate_collection_newicks(n_trees_small, n_leaves, tree_type="mixed")
    return Forest(newicks)


# ======================================================================== #
# 1. Construction benchmarks                                                #
# ======================================================================== #


class TestBenchConstruction:
    """
    Measure Forest construction time.

    Construction encompasses:
      - NEWICK string parsing
      - Euler tour computation
      - Sparse table (RMQ) construction
      - CSR array packing
      - Global taxon namespace assembly

    These benchmarks are parameterised over n_trees and n_leaves so that
    O(n_trees × n_nodes) scaling can be confirmed and the per-tree fixed
    overhead can be separated from the per-node variable cost.
    """

    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_SMALL],
        ids=[k for k, _ in TREE_COUNTS_SMALL],
    )
    def test_construction_balanced(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Construction cost for balanced binary trees."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="balanced"
        )
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "balanced",
                "n_nodes": 2 * n_leaves_val - 1,
            }
        )
        benchmark(Forest, newicks)

    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_SMALL],
        ids=[k for k, _ in TREE_COUNTS_SMALL],
    )
    def test_construction_caterpillar(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Construction cost for caterpillar (max-depth) trees."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="caterpillar"
        )
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "caterpillar",
                "n_nodes": 2 * n_leaves_val - 1,
            }
        )
        benchmark(Forest, newicks)

    @pytest.mark.large_scale
    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_LARGE],
        ids=[k for k, _ in TREE_COUNTS_LARGE],
    )
    def test_construction_large(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Large-scale construction — shows O(n_trees) scaling slope."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="mixed"
        )
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "mixed",
                "n_nodes": 2 * n_leaves_val - 1,
            }
        )
        benchmark(Forest, newicks)


# ======================================================================== #
# 2. Quartet topology benchmarks — counts only                              #
# ======================================================================== #


class TestBenchQuartetTopologyCounts:
    """
    Measure quartet_topology() in counts-only mode (steiner=False).

    The primary kernel cost is 6 RMQ calls per (quartet, tree) pair, each
    O(1) but with Python interpreter overhead at baseline.  These benchmarks
    expose three scaling axes:

      n_trees   — the inner loop count
      n_leaves  — affects sparse table size and LCA lookup cache behaviour
      n_quartets — the outer loop count (batch dimension)

    Tree type is "mixed" to avoid pathological cache effects from a perfectly
    uniform topology.
    """

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_balanced(
        self,
        benchmark,
        collection_balanced_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Counts-only on balanced trees; primary scaling benchmark."""
        c = collection_balanced_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "balanced",
            }
        )
        benchmark(c.quartet_topology, quartets)

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_caterpillar(
        self,
        benchmark,
        collection_caterpillar_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Counts-only on caterpillar trees; worst-case sparse table depth."""
        c = collection_caterpillar_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "caterpillar",
            }
        )
        benchmark(c.quartet_topology, quartets)


# ======================================================================== #
# 3. Quartet topology benchmarks — with Steiner distances                   #
# ======================================================================== #


class TestBenchQuartetTopologySteiner:
    """
    Measure quartet_topology() with steiner=True.

    The Steiner mode adds 4 root-distance reads and 9 float ops per active
    (quartet, tree) pair beyond the topology core, plus one write to the
    (n_quartets, n_trees, 3) output array.  These benchmarks quantify:

      - The absolute overhead of Steiner mode relative to counts-only.
      - How output-array size scales with n_quartets × n_trees (memory bandwidth).

    Steiner output arrays are pre-allocated by the public method so allocation
    cost is included in the measured time.
    """

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_balanced(
        self,
        benchmark,
        collection_balanced_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Steiner mode on balanced trees."""
        c = collection_balanced_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)
        steiner_output_bytes = n_q * n_trees_small * 3 * 8  # float64

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "balanced",
                "steiner_out_bytes": steiner_output_bytes,
            }
        )
        benchmark(c.quartet_topology, quartets, steiner=True)

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_caterpillar(
        self,
        benchmark,
        collection_caterpillar_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Steiner mode on caterpillar trees; checks depth sensitivity."""
        c = collection_caterpillar_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)
        steiner_output_bytes = n_q * n_trees_small * 3 * 8

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "caterpillar",
                "steiner_out_bytes": steiner_output_bytes,
            }
        )
        benchmark(c.quartet_topology, quartets, steiner=True)

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_overhead_vs_counts(
        self,
        benchmark,
        collection_mixed_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """
        Steiner mode on mixed trees — run alongside counts benchmark to
        directly compare timings for the same collection and quartet set.
        The ratio of these two results quantifies raw Steiner overhead.
        """
        c = collection_mixed_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "mixed",
                "mode": "steiner",
            }
        )
        benchmark(c.quartet_topology, quartets, steiner=True)

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_overhead_vs_steiner(
        self,
        benchmark,
        collection_mixed_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """
        Counts-only on the same mixed collection as test_steiner_overhead_vs_counts.
        Placing both in the same class makes --benchmark-compare diffs clean.
        """
        c = collection_mixed_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "mixed",
                "mode": "counts_only",
            }
        )
        benchmark(c.quartet_topology, quartets)


# ======================================================================== #
# 4. Branch distance benchmarks                                             #
# ======================================================================== #


class TestBenchBranchDistance:
    """
    Measure branch_distance() for a single taxon pair across all trees.

    branch_distance executes one RMQ call per tree where both taxa are
    present.  These benchmarks establish the per-RMQ baseline cost and
    confirm O(n_trees) scaling, which is the same inner loop that drives
    quartet_topology.

    A fixed taxon pair (the first two leaves, A and B) is always present in
    all generated trees, so there are no skipped trees to confound scaling.
    """

    # Always measure with the two leaves guaranteed present in every tree.
    TAXON_A: str = _leaf_name(0)  # "A"
    TAXON_B: str = _leaf_name(1)  # "B"

    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_SMALL],
        ids=[k for k, _ in TREE_COUNTS_SMALL],
    )
    def test_branch_distance_balanced(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Branch distance on balanced trees — baseline RMQ throughput."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="balanced"
        )
        c = Forest(newicks)
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "balanced",
            }
        )
        benchmark(c.branch_distance, self.TAXON_A, self.TAXON_B)

    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_SMALL],
        ids=[k for k, _ in TREE_COUNTS_SMALL],
    )
    def test_branch_distance_caterpillar(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Branch distance on caterpillar trees — worst-case LCA depth."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="caterpillar"
        )
        c = Forest(newicks)
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "caterpillar",
            }
        )
        benchmark(c.branch_distance, self.TAXON_A, self.TAXON_B)

    @pytest.mark.large_scale
    @pytest.mark.parametrize(
        "n_leaves_val,n_leaves_label",
        [(v, k) for k, v in LEAF_COUNTS],
        ids=[k for k, _ in LEAF_COUNTS],
    )
    @pytest.mark.parametrize(
        "n_trees_val,n_trees_label",
        [(v, k) for k, v in TREE_COUNTS_LARGE],
        ids=[k for k, _ in TREE_COUNTS_LARGE],
    )
    def test_branch_distance_large(
        self, benchmark, n_trees_val, n_trees_label, n_leaves_val, n_leaves_label
    ):
        """Large-scale branch distance — confirms O(n_trees) slope."""
        newicks = generate_collection_newicks(
            n_trees_val, n_leaves_val, tree_type="mixed"
        )
        c = Forest(newicks)
        benchmark.extra_info.update(
            {
                "n_trees": n_trees_val,
                "n_leaves": n_leaves_val,
                "tree_type": "mixed",
            }
        )
        benchmark(c.branch_distance, self.TAXON_A, self.TAXON_B)


# ======================================================================== #
# 5. Numba quartet topology benchmarks — counts only                        #
# ======================================================================== #

# All tests in this class are skipped automatically when numba is not
# installed.  The skip is applied at collection time via the class-level
# pytestmark so it appears cleanly in the benchmark report.


@pytest.mark.skipif(not _NUMBA_AVAILABLE, reason="numba not installed")
class TestBenchQuartetTopologyCountsCPU:
    """
    Measure quartet_topology(..., backend='cpu-parallel') in counts-only mode.

    The cpu-parallel path calls _quartet_counts_njit, which is decorated with
    @njit(parallel=True) and uses prange over the quartet axis.  On first
    invocation the kernel is JIT-compiled; all subsequent calls hit the
    compiled native code.

    JIT warmup
    ----------
    Each test calls the kernel once (via benchmark.pedantic's setup
    parameter) before the timed rounds begin.  This ensures the benchmark
    measures steady-state throughput rather than compilation time.
    Thread count is controlled by numba.set_num_threads() before running.

    Direct comparison with TestBenchQuartetTopologyCounts
    -----------------------------------------------------
    The test names, fixture parameters, and extra_info keys are kept
    identical to the Python baseline class so that --benchmark-compare
    produces clean side-by-side diffs.  The only difference is the
    backend keyword and the warmup step.
    """

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_balanced(
        self,
        benchmark,
        collection_balanced_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Counts-only cpu-parallel on balanced trees."""
        c = collection_balanced_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "balanced",
                "backend": "cpu-parallel",
            }
        )
        # Warm up the JIT before the timed rounds.
        c.quartet_topology(quartets, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, backend="cpu-parallel")

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_caterpillar(
        self,
        benchmark,
        collection_caterpillar_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Counts-only cpu-parallel on caterpillar trees."""
        c = collection_caterpillar_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "caterpillar",
                "backend": "cpu-parallel",
            }
        )
        c.quartet_topology(quartets, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, backend="cpu-parallel")


# ======================================================================== #
# 6. Numba quartet topology benchmarks — with Steiner distances             #
# ======================================================================== #


@pytest.mark.skipif(not _NUMBA_AVAILABLE, reason="numba not installed")
class TestBenchQuartetTopologySteinerCPU:
    """
    Measure quartet_topology(..., steiner=True, backend='cpu-parallel').

    Mirrors TestBenchQuartetTopologySteiner with the numba backend.
    JIT warmup is performed before each benchmark loop.
    """

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_balanced(
        self,
        benchmark,
        collection_balanced_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Steiner cpu-parallel on balanced trees."""
        c = collection_balanced_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)
        steiner_output_bytes = n_q * n_trees_small * 3 * 8

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "balanced",
                "steiner_out_bytes": steiner_output_bytes,
                "backend": "cpu-parallel",
            }
        )
        c.quartet_topology(quartets, steiner=True, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, steiner=True, backend="cpu-parallel")

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_caterpillar(
        self,
        benchmark,
        collection_caterpillar_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Steiner cpu-parallel on caterpillar trees."""
        c = collection_caterpillar_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)
        steiner_output_bytes = n_q * n_trees_small * 3 * 8

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "caterpillar",
                "steiner_out_bytes": steiner_output_bytes,
                "backend": "cpu-parallel",
            }
        )
        c.quartet_topology(quartets, steiner=True, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, steiner=True, backend="cpu-parallel")

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_steiner_overhead_vs_counts(
        self,
        benchmark,
        collection_mixed_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Steiner cpu-parallel on mixed trees — pairs with test_counts_overhead_vs_steiner."""
        c = collection_mixed_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "mixed",
                "mode": "steiner",
                "backend": "cpu-parallel",
            }
        )
        c.quartet_topology(quartets, steiner=True, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, steiner=True, backend="cpu-parallel")

    @pytest.mark.parametrize(
        "n_quartets_val,n_quartets_label",
        [(v, k) for k, v in QUARTET_COUNTS],
        ids=[k for k, _ in QUARTET_COUNTS],
    )
    def test_counts_overhead_vs_steiner(
        self,
        benchmark,
        collection_mixed_small,
        n_trees_small,
        n_leaves,
        n_quartets_val,
        n_quartets_label,
    ):
        """Counts-only cpu-parallel on mixed trees — pairs with test_steiner_overhead_vs_counts."""
        c = collection_mixed_small
        leaf_pool = c.global_names[
            : min(len(c.global_names), MAX_LEAVES_FOR_ALL_QUARTETS)
        ]
        quartets = generate_quartet_list(leaf_pool, n_quartets_val)
        n_q = len(quartets)

        benchmark.extra_info.update(
            {
                "n_trees": n_trees_small,
                "n_leaves": n_leaves,
                "n_quartets": n_q,
                "tree_type": "mixed",
                "mode": "counts_only",
                "backend": "cpu-parallel",
            }
        )
        c.quartet_topology(quartets, backend="cpu-parallel")
        benchmark(c.quartet_topology, quartets, backend="cpu-parallel")
